{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### algorithm based on human -> output\n",
    "#### SIFT,HOG(human invaded)-> machine learning(SVM,KNN)->output\n",
    "#### neural network(only)->output (deep learning = end-to-end machine learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오차제곱합\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sum_squares_error(y,t):\n",
    "    return 0.5 *np.sum((y-t)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "sum_squares_error(np.array(y),np.array(t))\n",
    "\n",
    "\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "sum_squares_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#교차 엔트로피 오차\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6306c28c48a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=\\\n",
    "load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=10\n",
    "batch_mask=np.random.choice(train_size,batch_size)\n",
    "x_batch=x_train[batch_mask]\n",
    "t_batch=t_train[batch_mask]\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        t=t.reshape(1,t.size)\n",
    "        y=y.reshape(1,y.size)\n",
    "        \n",
    "    batch_size=y.shape[0]\n",
    "    \n",
    "    #for one-hot coding\n",
    "    return -np.sum(t*np.log(y+1e-7))/batch_size\n",
    "    \n",
    "    #for label\n",
    "    #return -np.sum(np.log(y[np.arange(batch_size),t]+1e-7))/batch_size\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차함수\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "#미분함수\n",
    "def numerical_diff(f,x):\n",
    "    h=1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "\n",
    "#편미분 함수\n",
    "def function_2(x):\n",
    "    return x[0]**2 +x[1]**2\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "x=np.arange(0.0,20.0,0.1)\n",
    "y=function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x0=3, x1=4\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0*2.0\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0+x1*x1\n",
    "\n",
    "\n",
    "\n",
    "numerical_diff(function_tmp1,3.0)\n",
    "numerical_diff(function_tmp2,4.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    #x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val=x[idx]\n",
    "        \n",
    "        #f(x+h)계산\n",
    "        x[idx]=tmp_val+h\n",
    "        fxh1=f(x)\n",
    "        \n",
    "        #f(x-h)계산\n",
    "        x[idx]=tmp_val-h\n",
    "        fxh2=f(x)\n",
    "        \n",
    "        \n",
    "        grad[idx]=(fxh1-fxh2)/(2*h)\n",
    "        x[idx]=tmp_val#값 복원\n",
    "        \n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실 함수를 설정하는 이유?\n",
    "\n",
    "정확도라는 지표를 두고 손실 함수를 정의하는 이유는, 정확도는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없게 되고, 손실 함수의 미분은 음 /양 방향으로 수정할 수 있도록 정의할 수 있기 때문이다!(정확도는 불연속값이 발생할 수 있고, 손실 함수는 그렇지 않다..)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사법\n",
    "\n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 함수 값을 점차 줄이는 것이 경사법이다.\n",
    "\n",
    "## 경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법이다. 특히 신경망 학습에서는 경사법을 많이 사용한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f- 최적화 함수, init_x- 초깃값, lr- 학습률 , step_num - 갱신처리 횟수\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x=init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습에서도 기울기를 구해야한다. \n",
    "\n",
    "수식에서 하나의 원소가 의미하는 것은 w11을 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐를 보여주는 것이다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as py\n",
    "from common.functions import softmax,cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W=np.random.randn(2,3)\n",
    "        #정규분포초기화\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        z=self.predict(x)\n",
    "        y=softmax(z)\n",
    "        loss=cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47672163 1.05941932 0.98930138]\n",
      " [0.96472037 1.02020296 0.83584964]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1173898844187278"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net =simpleNet()\n",
    "print(net.W) #가중치 매개변수\n",
    "x=np.array([0.6,0.9])\n",
    "# p=net.predict(x)\n",
    "# print(p)\n",
    "\n",
    "#정답레이블\n",
    "\n",
    "t=np.array([0,0,1])\n",
    "net.loss(x,t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16206135  0.24165907 -0.40372042]\n",
      " [ 0.24309202  0.3624886  -0.60558062]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dW=numerical_gradient(f,net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 볼때 [w11 w12 w13\n",
    "          w21 w22 w23] 에서,\n",
    "w11을 h만큼 늘리면 손실 함수의 값은 0.16h만큼 증가하는 의미고, \n",
    "w23에서는 h만큼 늘리면 손실 함수의 값은 0.6h만큼 감소한다는 의미이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘 구현\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체\n",
    "\n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. 신경망 학습은 다음 4단계를 거친다.\n",
    "\n",
    "#### 1단계 - 미니배치\n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는게 목표\n",
    "\n",
    "#### 2단계 - 기울기 산출\n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 적게 하는 방향을 제시\n",
    "\n",
    "#### 3단계 - 매개변수 갱신\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "#### 4단계\n",
    "    1~3단계를 반복한다\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2층 신경망 클래스 구현\n",
    "\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # the number of input layer neuron, the number of hidden layer neuron , the number of output layer neuron\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        \n",
    "        self.params={}\n",
    "        # params=>신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수)\n",
    "        \n",
    "        self.params['W1']=weight_init_std* \\\n",
    "                          np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std* \\\n",
    "                          np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "        #추론을 수행함\n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'],self.params['W2']\n",
    "        b1,b2=self.params['b1'],self.params['b2']\n",
    "        \n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "    \n",
    "        return cross_entropy_error(y,t)\n",
    "        #정확도를 구함\n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        t=np.argmax(y,axis=1)\n",
    "        \n",
    "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "        #가중치 매개변수의 기울기를 구한다\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W=lambda W:self.loss(x,t)\n",
    "        \n",
    "        grads={}\n",
    "        grads['W1']=numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1']=numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2']=numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2']=numerical_gradient(loss_W,self.params['b2'])\n",
    "        \n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784,hidden_size=100,output_size=10)\n",
    "\n",
    "x=np.random.rand(100,784) #더미 입력 데이터(100장 분량)\n",
    "t=np.random.rand(100,10)  #더미 정답 레이블(100장 분량)\n",
    "\n",
    "#수치 미분 방식으로 매개변수의 기울기를 계산함\n",
    "\n",
    "grads=net.numerical_gradient(x,t)\n",
    "\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)\n",
    "\n",
    "#params 변수에는 이 신경망에 필요한 매개변수가 모두 저장됨을 보여준다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.rand(100,784)\n",
    "y=net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'two_layer_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-da88631c44f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwo_layer_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalisze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'two_layer_net'"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=\\\n",
    "    load_mnist(normalisze=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list=[]\n",
    "\n",
    "#hyperparameter\n",
    "\n",
    "iters_num=10000  #반복 홧수\n",
    "\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100   #미니배치크기\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #미니배치 획득\n",
    "    batch_mask=np.random.choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad=network.numerical_gradient(x_batch,t_batch)\n",
    "    #grad=network.gradient(x_batch,t_batch)#성능개선판!!!\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "        \n",
    "    #학습 경과 기록\n",
    "    loss=network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    \n",
    "#데이터를 반복해서 학습함으로 써 최적 가중치 매개변수로 서서히 다가서고 있다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "너무 과도하게 데이터에 대해 모델을 learning을 한 경우를 의미한다. 현재에 대해 잘 설명하는 것 만으로 충분하지 않다. 우리가 사실 원하는 정보는 기존에 알고 있는 데이터에 대한 것이 아니라 새롭게 우리가 알게되는 데이터에 대한 것들을 알고 싶은 것인데, 정작 새로운 데이터에 대해서는 하나도 못맞추고, 제대로 설명할 수 없는 경우라면 쓸모가 없음.\n",
    "\n",
    "오버피팅되었다는 것은, 예를 들어 훈련 데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻이다.\n",
    "\n",
    "### epoch\n",
    "\n",
    "에폭은 하나의 단위이다. 1 에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 예컨대 훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법은 100회 반복하면 모든 훈련 데이터를 '소진'한 게 된다. 이 경우 100회가 1에폭이 된다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'two_layer_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-86a7a3c90173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwo_layer_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'two_layer_net'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=\\\n",
    "    load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "#hyper parameter\n",
    "iters_num=10000\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "learning_rate=0.1\n",
    "\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "\n",
    "#1에폭당 반복수\n",
    "iter_per_epoch=max(train_size/batch_size,1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    \n",
    "    #미니배치 획득\n",
    "    batch_mask=np.random.choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad=network.numerical_gradient(x_batch,t_batch)\n",
    "    #grad = network.gradient(x_batch,t_batch) #성능 개선판!\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate*grad[key]\n",
    "        \n",
    "    #학습 경과 기록\n",
    "    loss=network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch==0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc=network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \"+ str(train_acc)+\",\"+str(test_acc))\n",
    "        \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
